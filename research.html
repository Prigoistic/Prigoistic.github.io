<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Priyam — Research</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;500;700&display=swap" rel="stylesheet" />
  <link rel="stylesheet" href="style.css" />
</head>
<body>

<div class="container">

  <!-- ═══════════ NAVBAR ═══════════ -->
  <nav>
    <a href="index.html">ABOUT</a><span class="sep">·</span><a href="projects.html">PROJECTS</a><span class="sep">·</span><a href="experience.html">EXPERIENCE</a><span class="sep">·</span><a href="research.html" class="active">RESEARCH</a><span class="sep">·</span><a href="blog.html">BLOG</a><span class="sep">·</span><a href="contact.html">CONTACT</a>
  </nav>

  <!-- ═══════════ PAGE HEADER ═══════════ -->
  <div class="page-header-row">
    <div class="page-header-text">
      <div class="page-title">RESEARCH</div>
      <div class="page-desc">
        Papers, ideas, and ongoing investigations.
      </div>
    </div>
<pre class="ascii-art">
             _.-;;-._
      '-..-'|   ||   |
      '-..-'|_.-;;-._|
      '-..-'|   ||   |
jgs   '-..-'|_.-''-._|
</pre>
  </div>

  <hr />

  <!-- ═══════════ RESEARCH AREAS ═══════════ -->
  <div class="section-title">AREAS</div>

  <div class="list-item">Energy based self learning engines </div>
  <div class="list-item">Reasoning and symbolic augmentation for Spatial Systems</div>
  <div class="list-item">Computational Neuroscience and Neural Modeling of Human Like Long Context Memory</div>
  <div class="list-item">Agent-centric AI architecture integrating long-term memory, adaptive retrieval, and reasoning pipelines</div>
  <div class="list-item">Controlled semantic drift modeling in persistent LLM memory architectures</div>

  <hr />

  <!-- ═══════════ PAPERS ═══════════ -->
  <div class="section-title">PAPERS</div>

  <!-- ── PAPER 01 (template — fill in your own) ── -->
  
  <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/400712116_Differential_Memory_Decay_A_Continuous-Time_Model_of_Forgetting_and_Semantic_Drift" target="_blank" rel="noopener">
        Differential Memory Decay: A Continuous-Time Model of Forgetting and Semantic Drift
      </a>
    </div>
    <div class="paper-authors">Priyam Ghosh, Krish Jasiwal, Sauhard Gupta ( MetaCognition Labs )</div>
    <div class="paper-venue">ICLR Workshop Submission 2026</div>
    <div class="paper-abstract">
This work introduces a unified, energy-based framework for building a self-evolving memory substrate for AI agents. Instead of treating memory as static storage, we model it as a continuous dynamical system that integrates associative recall, adaptive decay, semantic diffusion, competition, and replay into a single convergent formulation. The result is a scalable memory layer that reorganizes itself over time, enabling long-term stability, abstraction, and persistent identity in artificial systems.
    </div>
    <!-- <div class="paper-links">
      <a href="#" target="_blank" rel="noopener">arXiv</a>
      <a href="#" target="_blank" rel="noopener">PDF</a>
      <a href="#" target="_blank" rel="noopener">Code</a>
      <a href="#" target="_blank" rel="noopener">BibTeX</a>
    </div> -->
  </div>
  

  <!-- ── PAPER 02 (template) ── -->
  
  <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/397885858_An_Energy-Based_Self-Learning_Engine_for_Neuro-Symbolic_Scientific_Reasoning">An Energy-Based Self-Learning Engine for Neuro-Symbolic Scientific Reasoning</a>
    </div>
    <div class="paper-authors">Ananta Research</div>
    <div class="paper-venue">ICLR Workshop Submission 2026</div>
    <div class="paper-abstract">
      We introduce a recursive, energy-based self-learning architecture for neuro-symbolic scientific reasoning that tightly integrates a Hybrid Math-Text Tokenizer, a formal Recursive Logic Subsystem, and an Energy-Based Transformer. By coupling symbolic step-wise verification with energy minimization over derivation traces, the system forms a self-reinforcing loop that prioritizes process-level correctness, stability, and sample efficiency over raw likelihood. This framework provides a principled path toward scalable, verifiable scientific intelligence capable of autonomously formalizing and refining its own reasoning.

      
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>

    <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/395473790_A_Critical_Analysis_of_the_Proposed_Recursive_Logic_Subsystem_for_Self-Learning_LLMs_in_Scientific_Discovery">A Critical Analysis of the Proposed Recursive Logic Subsystem for Self-Learning LLMs in Scientific Discovery</a>
    </div>
    <div class="paper-authors">Ananta Research</div>
    <div class="paper-venue">ResearchGate</div>
    <div class="paper-abstract">
This analysis examines the proposed Recursive Logic Subsystem (RLS), a neuro-symbolic architecture that recursively integrates self-learning, formal verification, and dynamic curriculum design to enhance the reliability of scientific reasoning in LLMs. By coupling generative models with a logic engine in a co-evolutionary loop, the system aims to provide process-level verification, mitigate hallucinations, and expand formal knowledge through autoformalization. Despite significant challenges in computation, brittleness, and alignment, the RLS outlines a concrete roadmap toward robust, autonomous “Agentic Science.”

      
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>

      <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/393773297_Bridging_the_Semantic_Gap_A_Hybrid_Math-Text_Tokenizer_for_Enhanced_Logical_Reasoning_in_Large_Language_Models">Bridging the Semantic Gap: A Hybrid Math-Text Tokenizer for Enhanced Logical Reasoning in Large Language Models</a>
    </div>
    <div class="paper-authors">Ananta Research</div>
    <div class="paper-venue">ResearchGate</div>
    <div class="paper-abstract">
This paper argues that LLM brittleness in formal reasoning stems from a fundamental “tokenization bottleneck,” where subword schemes fragment mathematically meaningful structures. We propose the Hybrid Math-Text Tokenizer (HMTT), a structure-aware dual-stream tokenizer that preserves atomic formal units using LaTeX and AST parsing, significantly improving reasoning performance across math and code benchmarks. Empirically, HMTT boosts MATH accuracy by over 12 points and introduces a Tokenization Fidelity Score (TFS), strongly correlated with downstream reasoning gains.

      
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>

    <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/392438202_Ananta_A_Self-Learning_LLM_for_Symbolic_and_Scientific_Reasoning">Ananta: A Self-Learning LLM for Symbolic and Scientific Reasoning</a>
    </div>
    <div class="paper-authors">Ananta Research</div>
    <div class="paper-venue">ResearchGate</div>
    <div class="paper-abstract">
We introduce Ananta, a novel large language model (LLM) architecture designed for advanced symbolic and scientific reasoning. Ananta uniquely integrates self-supervised learning and reinforcement learning techniques to derive physical equations and generate algorithms from first principles. Its training leverages cu-rated datasets spanning mathematics and scientific derivations (e.g., DeepMind Mathematics , GSM8K, MathQA, and symbolic derivations from arXiv papers) to validate its reasoning abilities. Key innovations include symbolic curriculum learning for gradually increasing problem complexity, recursive logic model-ing to enable multi-step derivations, and hybrid math-text tokenization that treats mathematical symbols and expressions distinctly. We compare Ananta's architecture and fine-tuning (us-ing LoRA, RLHF, PPO) against leading models such as DeepSeek-R1, AlphaEvolve, and GPT-4. Extensive experiments on standard math and physics benchmarks demonstrate that Ananta achieves higher symbolic consistency and derivation completeness while maintaining competitive accuracy. We present architectural diagrams, training pipelines, and pseudocode for Ananta's algorithm derivation modules, along with evaluations on novel reasoning generation. Our results suggest that Ananta advances the state of the art in neural symbolic reasoning, opening avenues for AI-driven scientific discovery.
      
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>

  <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/391653563_Explicit_Temporal_Attention_in_Video_Diffusion_Models">Explicit Temporal Attention in Video Diffusion Models</a>
    </div>
    <div class="paper-authors">Priyam Ghosh</div>
    <div class="paper-venue">CVPRW 2024</div>
    <div class="paper-abstract">
We propose an Explicit Temporal Attention mechanism for video diffusion models that enhances temporal coherence and sample quality in video generation tasks. Our approach extends standard 3D U-Net diffusion architectures by inserting specialized attention blocks that explicitly compute attention across the temporal dimension. We derive the full forward and reverse diffusion equations and incorporate multi-frame conditioning via temporal attention. Experiments on UCF-101 and Kinetics-600 show that our method achieves superior Fréchet Video Distance (FVD) and lower Temporal-MSE compared to leading baselines (e.g., Video Diffusion [1], W.A.L.T [3]), demonstrating improved frame-to-frame consistency. We provide extensive implementation details (training time, hardware, hyperparameters) and ablation studies, and discuss limitations and future work. Pseudocode is given for the training loop, temporal attention block, and denoising forward pass.      
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>

    <div class="paper">
    <div class="paper-title">
      <a href="https://www.researchgate.net/publication/391628259_Developing_an_Autonomous_System_for_Identification_and_Removal_of_Microplastic_from_Water_Bodies_M2D2">Developing an Autonomous System for Identification and Removal of Microplastic from Water Bodies (M2D2)</a>
    </div>
    <div class="paper-authors">Priyam Ghosh, Siddhant Shivam ( Elysium )</div>
    <div class="paper-venue">Earth Prize 2022, Global Talent Week 2022</div>
    <div class="paper-abstract">
      Microplastics (< 5 mm) pose serious ecological and health risks by entering aquatic food chains and carrying toxic pollutants. This paper presents M2D2 (Mobile Microplastic Diminution Device), an autonomous system that (1) collects microplastics via a Bulk Acoustic Wave (BAW) microfluidic module, (2) identifies and quantifies them using a Digital In-line Holographic Microscope (DIHM) with 3D reconstruction, and (3) navigates toward high-concentration regions using onboard processing (Raspberry Pi) with a CNN+RIHVR feedback loop. In laboratory tests, the BAW module effectively focused and extracted microplastic particles into a central outlet, in agreement with theoretical models of acoustophoretic force (e.g., particles with positive acoustic contrast factor move to pressure nodes). The DIHM captured interference patterns of particles in water, enabling 3D reconstruction of particle fields. A convolutional neural network trained on RIHVR-processed holograms achieved rapid concentration mapping, with predictions closely correlating (≈ 0.95) with full RIHVR results. Overall, the integrated prototype demonstrated high collection efficiency for particles above a few microns and accurate identification of particle fields. This approach is cost-effective (uses inexpensive hardware) and scalable, offering a promising solution to reduce microplastic pollution in waters
    </div>
    <!-- <div class="paper-links">
      <a href="#">arXiv</a>
      <a href="#">PDF</a>
    </div> -->
  </div>


  <div style="font-size: 13px; color: #555555;">
    Papers will be listed here as they are published.
  </div>

  <hr />

  <!-- ═══════════ NOTES / WRITINGS ═══════════ -->
  <div class="section-title">NOTES</div>

  <div style="font-size: 13px; color: #555555; margin-bottom: 24px;">
    Technical notes and informal write-ups.
  </div>

  <!-- ── NOTE TEMPLATE ── -->
  <!--
  <div class="paper">
    <div class="paper-title">
      <a href="#">Title of Technical Note</a>
    </div>
    <div class="paper-venue">February 2026</div>
    <div class="paper-abstract">
      Brief summary of the note's content.
    </div>
    <div class="paper-links">
      <a href="#">Read</a>
      <a href="#">PDF</a>
    </div>
  </div>
  -->

  <div class="paper">
    <div class="paper-title">forgetting is doing the heavy lifting</div>
    <div class="paper-venue">Feb 2026 · ramble</div>
    <div class="paper-abstract">
      was reading some old neuroscience notes at 2am and had this moment. we keep trying to make AI remember everything and it's kind of the wrong instinct? like your brain actively throws stuff away and that's WHY you can think clearly. that's literally what our memory decay paper is about. I kept arguing with krish about whether drift was a bug or a feature and somewhere around the third whiteboard session it clicked. it's the feature. the fact that memories blur and merge is what gives you abstractions. a KV cache that never forgets isn't memory, it's a pile.
    </div>
  </div>

  <div class="paper">
    <div class="paper-title">the tokenizer was the problem the whole time</div>
    <div class="paper-venue">Jan 2026 · rant</div>
    <div class="paper-abstract">
      ok so we were debugging this thing for weeks. model keeps getting basic integrals wrong, like embarrassingly wrong. tried everything. more data, better prompts, reward shaping, nothing. then one night I just dumped the raw token IDs and saw that BPE was splitting "∫" and "dx" into completely unrelated fragments. the model never even got a chance. it was literally solving a jigsaw puzzle where someone cut the pieces wrong. that's when we said screw it and built HMTT from scratch. honestly the whole paper came from staring at tokenizer outputs and going "wait, THAT'S what it sees?"
    </div>
  </div>

  <div class="paper">
    <div class="paper-title">why I keep going back to energy models</div>
    <div class="paper-venue">Dec 2025 · thought</div>
    <div class="paper-abstract">
      I don't know how to explain this without sounding pretentious but when we rewrote ananta's objective from cross-entropy to energy minimization, something shifted in how I think about all of this. loss functions feel like accounting. energy feels like physics. like you're not telling the model "get closer to this answer," you're saying "find the stable configuration." and suddenly things like correctness and robustness stop being separate goals, they're just what low-energy states look like. I know that sounds hand-wavy. I'm still trying to formalize the intuition properly. but it changed something for me.
    </div>
  </div>

  <div class="paper">
    <div class="paper-title">your agent doesn't remember you</div>
    <div class="paper-venue">Nov 2025 · note</div>
    <div class="paper-abstract">
      tried using a bunch of agent frameworks last month and it hit me how hollow they feel. you talk to them for an hour, close the tab, come back and they have no idea who you are. no continuity. no "oh yeah we were working on that thing." it's like groundhog day every session. and people want to fix this by making the context window bigger? that's not memory, that's cramming. real memory is messy. it decays, it reorganizes, it connects things that happened months apart. that's what I want to build. not a bigger notepad, an actual sense of "I've been here before."
    </div>
  </div>

  <div class="paper">
    <div class="paper-title">the time we built a robot to eat microplastics</div>
    <div class="paper-venue">Oct 2025 · reflection</div>
    <div class="paper-abstract">
      someone once looked at my publication list and was like "why is there a microplastics paper between two AI papers" lol fair. but that project with siddhant was probably the most fun I've had doing research. we literally strapped a holographic microscope and a raspberry pi onto a thing that floats, wrote a CNN to tell it where the plastic is, and used acoustic waves to suck it up. half the time we were debugging water leaks not code. presented it at global talent week and people kept asking if it was real. it was real. it also broke twice during the demo. anyway I think the best research training is building something physical that can fail in ways your simulator never warned you about.
    </div>
  </div>

  <!-- ═══════════════════════════════════
       TO ADD A PAPER:
       1. Uncomment a paper template above
       2. Fill in: paper-title (with link), paper-authors,
          paper-venue, paper-abstract, paper-links
       3. Duplicate the block for more papers

       TO ADD IMAGES / FIGURES:
       <div class="gallery">
         <img src="images/figure.png" alt="Figure description" />
       </div>
       ═══════════════════════════════════ -->

  <hr />

  <!-- ═══════════ FOOTER ═══════════ -->
  <div class="footer">
    <div>&copy; 2026 Priyam</div>
    <div class="footer-links">
      <a href="https://linkedin.com/in/prigoistic" target="_blank" rel="noopener">LinkedIn</a><span class="sep">·</span><a href="https://x.com/prigoistic" target="_blank" rel="noopener">X</a><span class="sep">·</span><a href="https://github.com/prigoistic" target="_blank" rel="noopener">GitHub</a><span class="sep">·</span><a href="mailto:priyamghosh9753@gmail.com">Email</a>
    </div>
  </div>

</div>

</body>
</html>
